# This is the main configuration file for the application.
# ~~~~~

# Secret key
# ~~~~~
# The secret key is used to secure cryptographics functions.
# If you deploy your application to several instances be sure to use the same key!
application.secret = "nTnOIy6^yFM5o[Z_T6jBriIYm7id43TSeLJC1U?bxt?PhfMJeCYX@s;RcNqX]xeA"

# The application languages
# ~~~~~
application.langs = "en"

# Global object class
# ~~~~~
# Define the Global object class for this application.
# Default to Global in the root package.
# application.global=Global

# Router
# ~~~~~
# Define the Router object to use for this application.
# This router will be looked up first when the application is starting up,
# so make sure this is the entry point.
# Furthermore, it's assumed your route file is named properly.
# So for an application router like `my.application.Router`,
# you may need to define a router file `conf/my.application.routes`.
# Default to Routes in the root package (and conf/routes)
# application.router=my.application.Routes


# Logger
# ~~~~~
# You can also configure logback (http://logback.qos.ch/),
# by providing an application-logger.xml file in the conf directory.

# Root logger:
logger.root = ERROR

# Logger used by the framework:
logger.play = INFO

# Logger provided to your application:
logger.application = DEBUG

manager {
  notebooks {
    ###
    # Server dir (containing notebook files)
    dir = ./notebooks
    dir = ${?NOTEBOOKS_DIR}

    ###
    # Default custom configuration for all **CREATED** notebooks
    #
    # custom {
    #   localRepo  = "/tmp/repo",
    #   repos      = ["m" %  "default" % "http://repo.maven.apache.org/maven2/" % "maven"],
    #   deps       = ["junit % junit % 4.12"],
    #   imports    = ["import scala.util._"],
    #   sparkConf  = {
    #     spark.number.works.too : "test",
    #     spark.boh: 2
    #   }
    # }

    ###
    # Override custom configuration for **ALL** notebooks
    #  TO USE WITH CARE → could break full reproductability
    #
    # override {
    #   localRepo  = "/tmp/repo",
    #   repos      = ["m" %  "default" % "http://repo.maven.apache.org/maven2/" % "maven"],
    #   deps       = ["junit % junit % 4.12"],
    #   imports    = ["import scala.util._"],
    #   sparkConf  = {
    #     spark.number.works.too : "test",
    #     spark.boh: 2
    #   }
    # }
  }

  ###
  # Static resources to be made available on the web server
  # You may add your own resource directories
  # Paths may be relative to the server root, or absolute.
  #resources=["./my-resources"]

  ##
  # Name of SparkNotebook
  name = "Spark Notebook"

  ##
  #
  maxBytesInFlight = 5M

  kernel {
    ###
    # Uncomment to kill kernel after inactivity timeout
    # killTimeout = 60 minute

    ###
    # Uncomment to not to start the kernel (spark-context) automatically when notebook was open
    ## autostartOnNotebookOpen = false

    ###
    # Uncomment to enable remote vm debugging on the provided port
    #
    #debug.port=9090

    ###
    # Change the level of debug in the logs/sn-session-$kernelId-$notebookPath.log file
    #
    #log.level=debug

    ###
    # Add vmArgs to the remote process
    #vmArgs=["-XX:+PrintGCDetails", "-XX:+PrintGCDetails", "-Dsun.io.serialization.extendedDebugInfo=true"]

    ###
    # Working directory for kernel VMs
    #dir=.


    ###
    # List of URLs of kernel init scripts (to be run when a kernel first starts).
    #init=[]

    ###
    # Kernel VM memory settings
    #heap=4g
    #stack=-1 #default XSS
    permGen = 1024m
    #reservedCodeCache=-1 #default

    ###
    # Classpath for kernel VMs (defaults to server VM classpath)
    #classpath=[]

    ###
    # REPL compiler options: Use the deprecation warning by default for more
    # useful feedback about obsolete functions, etc.
    # REPL compiler options: Use the -deprecation warning by default for more
    # useful feedback about obsolete functions, etc. Use the -feature warning
    # for more explicit warnings about "optional" language features that
    # should be enabled explicitly. One of those that's used by Spark Notebook
    # itself is "reflective calls".
    compilerArgs=["-deprecation", "-feature", "-language:reflectiveCalls"]
  }

  clusters {
    #profiles=./conf/profiles
    #file=./conf/clusters
  }
}

notebook-server {
  akka {
    loggers = ["akka.event.slf4j.Slf4jLogger"]
    loglevel = "DEBUG"
    stdout-loglevel = "DEBUG"

    log-config-on-start = off

    daemonic = true

    debug {
      ## enable function of LoggingReceive, which is to log any received message at DEBUG level
      # receive = on
      ## enable DEBUG logging of all AutoReceiveMessages (Kill, PoisonPill and the like)
      # autoreceive = on
      ## enable DEBUG logging of actor lifecycle changes
      # lifecycle = on
    }

    actor {
      provider = "akka.remote.RemoteActorRefProvider"

      default-stash-dispatcher {
        mailbox-type = "akka.dispatch.UnboundedDequeBasedMailbox"
      }
    }

    remote {
      ## Debugging:
      # log-sent-messages = on
      # log-received-messages = on

      enabled-transports = ["akka.remote.netty.tcp"]
      # transport = "akka.remote.netty.NettyRemoteTransport"

      ## see (http://doc.akka.io/docs/akka/snapshot/scala/remoting.html)
      # These configuration will help getting of akka timeouts, specially
      # → threshold                  (12  for instance, it's refered as a good choice on ec2)
      # → heartbeat-interval         (10s to reduce the number of comm between the server and the notebook backend)
      # → acceptable-heartbeat-pause (90s to reduce the number of comm between the server and the notebook backend)
      #
      #transport-failure-detector.heartbeat-interval = 4 s
      #transport-failure-detector.threshold = 7.0            # raise it to 12 for instance on EC2/Mesos/Yarn/...
      #transport-failure-detector.max-sample-size = 100
      #transport-failure-detector.min-std-deviation = 100 ms
      #transport-failure-detector.acceptable-heartbeat-pause = 10 s
      #watch-failure-detector.heartbeat-interval = 1 s
      #watch-failure-detector.threshold = 10.0               # raise it to 12 for instance on EC2/Mesos/Yarn/...
      #watch-failure-detector.max-sample-size = 200
      #watch-failure-detector.min-std-deviation = 100 ms
      #watch-failure-detector.acceptable-heartbeat-pause = 10 s
      #watch-failure-detector.unreachable-nodes-reaper-interval = 1s
      #watch-failure-detector.expected-response-after = 3 s

      netty.tcp {
        hostname = "127.0.0.1"
        port = 0

        maximum-frame-size = "1 GiB"
      }
    }
  }
}

# http {
#   proxyHost = ...
#   proxyPort = ...
#   proxyUser = ...
#   proxyPassword = ...
#   nonProxyHosts = ...
# }

remote-repos {
  proxy {
    # "protocol" = ...,
    # "host"     = ...,
    # "port"     = ...,
    # "username" = ...,
    # "password" = ...,
    # "nonProxyHosts" = ...
  }
}
