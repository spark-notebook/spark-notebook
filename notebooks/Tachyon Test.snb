{
  "metadata" : {
    "name" : "Tachyon Test",
    "user_save_timestamp" : "1970-01-01T01:00:00.000Z",
    "auto_save_timestamp" : "1970-01-01T01:00:00.000Z",
    "language_info" : {
      "name" : "scala",
      "file_extension" : "scala",
      "codemirror_mode" : "text/x-scala"
    },
    "trusted" : true,
    "customLocalRepo" : null,
    "customRepos" : null,
    "customDeps" : null,
    "customImports" : null,
    "customArgs" : null,
    "customSparkConf" : null
  },
  "cells" : [ {
    "metadata" : { },
    "cell_type" : "markdown",
    "source" : "# Use Tachyon has the local share tool (WIP)"
  }, {
    "metadata" : { },
    "cell_type" : "markdown",
    "source" : "## Context"
  }, {
    "metadata" : { },
    "cell_type" : "markdown",
    "source" : "This notebook aims to show how the local tachyon that is started automatically with the notebook can be used in order to cache data OFF_HEAP and can either be:\n* reused in a current notebook\n* or more interestingly here, reused from within **another** notebook"
  }, {
    "metadata" : { },
    "cell_type" : "markdown",
    "source" : "<span style=\"color:red\">/!\\</span> The second, and hopefully ephemeral, aim of this notebook is to show what is currently possible and asks the **Tachyon experts** what can be the next step to make the cached data reusable easily. \n\nThat's why there is an important <span style=\"font-size:14pt; color: darkgreen;\">Questions</span> section at the end."
  }, {
    "metadata" : { },
    "cell_type" : "markdown",
    "source" : "**IMPORTANT**: you should have started the notebook server with <span style=\"font-size:18pt;\">enough memory</span> to have tachyon working fine, if <span style=\"font-size:18pt;\">not, please restart</span> the notebook server this way (for `8G` of memory):\n```\nsbt -mem 8000\n```"
  }, {
    "metadata" : { },
    "cell_type" : "markdown",
    "source" : "## Usage"
  }, {
    "metadata" : { },
    "cell_type" : "markdown",
    "source" : "Running the code can be done is several ways:\n* 1 by 1: using `CTRL+Return`\n* 1 by 1 and going to the next using `SHIFT+Return`\n* all at once from the menu: `Cell` > `Run All`"
  }, {
    "metadata" : { },
    "cell_type" : "markdown",
    "source" : "## Env"
  }, {
    "metadata" : { },
    "cell_type" : "markdown",
    "source" : "In the next block we can see the current spark configuration, and specifically the properties:\n* spark.tachyonStore.url\n* spark.tachyonStore.url\n* spark.tachyonStore.folderName"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false
    },
    "cell_type" : "code",
    "source" : "sparkContext.getConf.toDebugString",
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "res2: String = \nspark.app.id=local-1440638317493\nspark.app.name=Tachyon Test\nspark.driver.host=192.168.1.144\nspark.driver.port=54503\nspark.executor.id=driver\nspark.externalBlockStore.baseDir=/share\nspark.externalBlockStore.folderName=spark-2f600e88-0ad6-4cf8-b60a-438bfdbd33c7\nspark.externalBlockStore.url=tachyon://172.17.42.1:36586\nspark.fileserver.uri=http://192.168.1.144:54338\nspark.jars=\nspark.master=local[*]\nspark.repl.class.uri=http://192.168.1.144:34986\nspark.tachyonStore.baseDir=/share\nspark.tachyonStore.url=tachyon://172.17.42.1:36586\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : "spark.app.id=local-1440638317493\nspark.app.name=Tachyon Test\nspark.driver.host=192.168.1.144\nspark.driver.port=54503\nspark.executor.id=driver\nspark.externalBlockStore.baseDir=/share\nspark.externalBlockStore.folderName=spark-2f600e88-0ad6-4cf8-b60a-438bfdbd33c7\nspark.externalBlockStore.url=tachyon://172.17.42.1:36586\nspark.fileserver.uri=http://192.168.1.144:54338\nspark.jars=\nspark.master=local[*]\nspark.repl.class.uri=http://192.168.1.144:34986\nspark.tachyonStore.baseDir=/share\nspark.tachyonStore.url=tachyon://172.17.42.1:36586"
      },
      "output_type" : "execute_result",
      "execution_count" : 1
    } ]
  }, {
    "metadata" : { },
    "cell_type" : "markdown",
    "source" : "## Experiment"
  }, {
    "metadata" : { },
    "cell_type" : "markdown",
    "source" : "### Download the data locally (regular csv file on stock prices)"
  }, {
    "metadata" : { },
    "cell_type" : "markdown",
    "source" : "<p style=\"color:red\"><strong>IMPORTANT:</strong> you will need</p>\n\n* the `wget` tool to download the file\n* a writeable `/tmp` folder where the file will be downloaded by default\n* `174Mb` of free disk"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false
    },
    "cell_type" : "code",
    "source" : "val dataUrl = \"https://s3-eu-west-1.amazonaws.com/spark-notebook-data/closes.csv\"\nval dataLocalDowloadPath = \"/tmp/closes.csv\"",
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "dataUrl: String = https://s3-eu-west-1.amazonaws.com/spark-notebook-data/closes.csv\ndataLocalDowloadPath: String = /tmp/closes.csv\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : "/tmp/closes.csv"
      },
      "output_type" : "execute_result",
      "execution_count" : 2
    } ]
  }, {
    "metadata" : { },
    "cell_type" : "markdown",
    "source" : "**Recall: this can take some time → downloading from s3 a file of 174M**"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : true
    },
    "cell_type" : "code",
    "source" : ":sh wget $dataUrl -O $dataLocalDowloadPath",
    "outputs" : [ ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false
    },
    "cell_type" : "code",
    "source" : "val dataFile = dataLocalDowloadPath",
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "dataFile: String = /tmp/closes.csv\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : "/tmp/closes.csv"
      },
      "output_type" : "execute_result",
      "execution_count" : 3
    } ]
  }, {
    "metadata" : { },
    "cell_type" : "markdown",
    "source" : "Let's check the number of lines in the file"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false
    },
    "cell_type" : "code",
    "source" : ":sh wc -l $dataFile",
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "warning: there were 1 feature warning(s); re-run with -feature for details\nimport sys.process._\nres3: String = \n\"8655036 /tmp/closes.csv\n\"\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/plain" : "8655036 /tmp/closes.csv\n"
      },
      "output_type" : "execute_result",
      "execution_count" : 4
    } ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : true
    },
    "cell_type" : "markdown",
    "source" : "### Reading data and cache it as usual"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false
    },
    "cell_type" : "code",
    "source" : "val rdd = sparkContext.textFile(\"file://\" + dataFile).cache",
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "rdd: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[1] at textFile at <console>:54\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : "MapPartitionsRDD[1] at textFile at &lt;console&gt;:54"
      },
      "output_type" : "execute_result",
      "execution_count" : 5
    } ]
  }, {
    "metadata" : { },
    "cell_type" : "markdown",
    "source" : "Dummy computation to force the cache"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false
    },
    "cell_type" : "code",
    "source" : "rdd.flatMap(_.filter(_ == '0')).map(_.toString.toInt).sum",
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "res4: Double = 0.0\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : "0.0"
      },
      "output_type" : "execute_result",
      "execution_count" : 6
    } ]
  }, {
    "metadata" : { },
    "cell_type" : "markdown",
    "source" : "At this stage, we can check [http://localhost:4040/storage](http://localhost:4040/storage) and we'll see that (depending on the available memory) a fraction if not the whole file has been cached."
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : true
    },
    "cell_type" : "markdown",
    "source" : "### Now we use Tachyon to cache"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false
    },
    "cell_type" : "code",
    "source" : "import org.apache.spark.storage.StorageLevel._\nval rdd2 = sparkContext.textFile(\"file://\" + dataFile).persist(OFF_HEAP)",
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "import org.apache.spark.storage.StorageLevel._\nrdd2: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[6] at textFile at <console>:55\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : "MapPartitionsRDD[6] at textFile at &lt;console&gt;:55"
      },
      "output_type" : "execute_result",
      "execution_count" : 7
    } ]
  }, {
    "metadata" : { },
    "cell_type" : "markdown",
    "source" : "Dummy computation to force the cache"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false
    },
    "cell_type" : "code",
    "source" : "rdd2.flatMap(_.filter(_ == '0')).map(_.toString.toInt).sum",
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "res5: Double = 0.0\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : "0.0"
      },
      "output_type" : "execute_result",
      "execution_count" : 8
    } ]
  }, {
    "metadata" : { },
    "cell_type" : "markdown",
    "source" : "Now, we can check [http://localhost:4040/storage](http://localhost:4040/storage) **again** and <span style=\"color:darkred\">Yeepee</span> the data has been cached in tachyon (_see `size in tachyon`_ column)."
  }, {
    "metadata" : { },
    "cell_type" : "markdown",
    "source" : "### Let's explore the Tachyon's content"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false
    },
    "cell_type" : "code",
    "source" : "import tachyon.client.TachyonFS\nval tachyonUrl = sparkContext.getConf.get(\"spark.externalBlockStore.url\")\nval shareDir = sparkContext.getConf.get(\"spark.externalBlockStore.baseDir\")\nval tachyonClient = TachyonFS.get(tachyonUrl)",
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "<console>:56: warning: method get in object TachyonFS is deprecated: see corresponding Javadoc for more information.\n       val tachyonClient = TachyonFS.get(tachyonUrl)\n                                     ^\nimport tachyon.client.TachyonFS\ntachyonUrl: String = tachyon://172.17.42.1:36586\nshareDir: String = /share\ntachyonClient: tachyon.client.TachyonFS = tachyon://172.17.42.1/172.17.42.1:36586\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : "tachyon://172.17.42.1/172.17.42.1:36586"
      },
      "output_type" : "execute_result",
      "execution_count" : 9
    } ]
  }, {
    "metadata" : { },
    "cell_type" : "markdown",
    "source" : "<strong>Here is the content of the `share` dir of Spark</strong>"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false
    },
    "cell_type" : "code",
    "source" : "import scala.collection.JavaConverters._\nval shared = tachyonClient.listStatus(new tachyon.TachyonURI(shareDir)).asScala.map(_.getPath)",
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "import scala.collection.JavaConverters._\nshared: scala.collection.mutable.Buffer[String] = ArrayBuffer(/share/spark-2f600e88-0ad6-4cf8-b60a-438bfdbd33c7)\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : "<div class=\"container-fluid\"><div><div class=\"col-md-12\"><div>\n      <script data-this=\"{&quot;dataId&quot;:&quot;anon33896b020183b771f6a6b2c9fc71ff9f&quot;,&quot;dataInit&quot;:[{&quot;string value&quot;:&quot;/share/spark-2f600e88-0ad6-4cf8-b60a-438bfdbd33c7&quot;}],&quot;genId&quot;:&quot;1902780383&quot;}\" type=\"text/x-scoped-javascript\">/*<![CDATA[*/req(['../javascripts/notebook/playground','../javascripts/notebook/magic/tableChart'], \n      function(playground, _magictableChart) {\n        // data ==> data-this (in observable.js's scopedEval) ==> this in JS => { dataId, dataInit, ... }\n        // this ==> scope (in observable.js's scopedEval) ==> this.parentElement ==> div.container below (toHtml)\n\n        playground.call(data,\n                        this\n                        ,\n                        {\n    \"f\": _magictableChart,\n    \"o\": {\"headers\":[\"string value\"],\"nrow\":1,\"shown\":1,\"width\":600,\"height\":400}\n  }\n  \n                        \n                        \n                      );\n      }\n    );/*]]>*/</script>\n    </div></div></div></div>"
      },
      "output_type" : "execute_result",
      "execution_count" : 10
    } ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false
    },
    "cell_type" : "code",
    "source" : "val currentCache = shared(0)",
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "currentCache: String = /share/spark-ffb1e292-8ef3-480b-9b08-c795736ddf04\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : "/share/spark-ffb1e292-8ef3-480b-9b08-c795736ddf04"
      },
      "output_type" : "execute_result",
      "execution_count" : 9
    } ]
  }, {
    "metadata" : { },
    "cell_type" : "markdown",
    "source" : "<strong>AFAICT, spark will use `<driver>` as the cache folder</strong>"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false
    },
    "cell_type" : "code",
    "source" : "val currentFiles = tachyonClient.listStatus(new tachyon.TachyonURI(currentCache+\"/driver\")).asScala.map(_.getPath)",
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "currentFiles: scala.collection.mutable.Buffer[String] = ArrayBuffer(/share/spark-ffb1e292-8ef3-480b-9b08-c795736ddf04/driver/spark-tachyon-20150827021746-6d0f)\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : "<div class=\"container-fluid\"><div><div class=\"col-md-12\"><div>\n      <script data-this=\"{&quot;dataId&quot;:&quot;anonfb12216b9bbb4e92ec4ad6f7b607756a&quot;,&quot;dataInit&quot;:[{&quot;string value&quot;:&quot;/share/spark-ffb1e292-8ef3-480b-9b08-c795736ddf04/driver/spark-tachyon-20150827021746-6d0f&quot;}],&quot;genId&quot;:&quot;1599447085&quot;}\" type=\"text/x-scoped-javascript\">/*<![CDATA[*/req(['../javascripts/notebook/playground','../javascripts/notebook/magic/tableChart'], \n      function(playground, _magictableChart) {\n        // data ==> data-this (in observable.js's scopedEval) ==> this in JS => { dataId, dataInit, ... }\n        // this ==> scope (in observable.js's scopedEval) ==> this.parentElement ==> div.container below (toHtml)\n\n        playground.call(data,\n                        this\n                        ,\n                        {\n    \"f\": _magictableChart,\n    \"o\": {\"headers\":[\"string value\"],\"nrow\":1,\"shown\":1,\"width\":600,\"height\":400}\n  }\n  \n                        \n                        \n                      );\n      }\n    );/*]]>*/</script>\n    </div></div></div></div>"
      },
      "output_type" : "execute_result",
      "execution_count" : 10
    } ]
  }, {
    "metadata" : { },
    "cell_type" : "markdown",
    "source" : "##### Here is the RDD that has been cached! → _ala_ hadoop with each partition written in its own folder and file part"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : true
    },
    "cell_type" : "code",
    "source" : "val thisRdd = currentFiles(0)\nval whatsInCurrentThisRdd = tachyonClient.listStatus(new tachyon.TachyonURI(thisRdd)).asScala.map(_.getPath)",
    "outputs" : [ ]
  }, {
    "metadata" : { },
    "cell_type" : "markdown",
    "source" : "Above we see the cached folders and partitions"
  }, {
    "metadata" : { },
    "cell_type" : "markdown",
    "source" : "<h2 id=\"questions\" name=\"questions\">QUESTIONS :-D</h2>"
  }, {
    "metadata" : { },
    "cell_type" : "markdown",
    "source" : "<p style=\"color:red;\">How to not do '/*'?</p>"
  }, {
    "metadata" : { },
    "cell_type" : "markdown",
    "source" : "So far, to load the whole data set back from the tachyon cache, we do\n```\nval tachyonThisFileRdd = sparkContext.textFile(tachyonUrl + thisRdd + \"/*\")\n```\n\nWhich is not exaclty what we should do I guess, I mean the `/*` shouldn't be required?"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : true
    },
    "cell_type" : "code",
    "source" : "val tachyonThisFileRdd = sparkContext.textFile(tachyonUrl + thisRdd + \"/*\")",
    "outputs" : [ ]
  }, {
    "metadata" : { },
    "cell_type" : "markdown",
    "source" : "<p style=\"color:red;\">The following will get the whole file as the result</p>"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : true
    },
    "cell_type" : "code",
    "source" : "//  tachyonThisFileRdd.take(1).head",
    "outputs" : [ ]
  }, {
    "metadata" : { },
    "cell_type" : "markdown",
    "source" : "<p style=\"color:red\">How to have the returned `RDD` having elements being a line rather than the whole file?</p>"
  } ],
  "nbformat" : 4
}